{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQmsrPG3Ld_r"
      },
      "source": [
        "#Reddit Fake Post Detection(by Looking Only at the Title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR6aqvRALl-N"
      },
      "source": [
        "##Problem Formulation\n",
        "False information on the Internet has caused many social problems due to the raise of social network and its role in different domains such as politics. In this project, we are going to predict if a specific reddit post is fake news or not, by looking at its title."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwTx3-GjL6ZF"
      },
      "source": [
        "## Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAc4eiA5_vJO",
        "outputId": "9caaa7ed-6075-418f-b6ae-c26c3cb6b136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSl641G_xIO"
      },
      "outputs": [],
      "source": [
        "#import libiraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#read train and test data\n",
        "data=pd.read_csv('/content/drive/MyDrive/xy_train[1].csv',index_col='id')\n",
        "test_=pd.read_csv('/content/drive/MyDrive/x_test[1].csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmP9SWOU_79G",
        "outputId": "c23a5cc8-301d-46f7-a97a-1e643db47dad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5640b6b5-97c5-443c-a879-22195e43f597\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265723</th>\n",
              "      <td>A group of friends began to volunteer at a hom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284269</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207715</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551106</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8584</th>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70046</th>\n",
              "      <td>Finish Sniper Simo H盲yh盲 during the invasion o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189377</th>\n",
              "      <td>Nigerian Prince Scam took $110K from Kansas ma...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93486</th>\n",
              "      <td>Is It Safe To Smoke Marijuana During Pregnancy...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140950</th>\n",
              "      <td>Julius Caesar upon realizing that everyone in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34509</th>\n",
              "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5640b6b5-97c5-443c-a879-22195e43f597')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5640b6b5-97c5-443c-a879-22195e43f597 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5640b6b5-97c5-443c-a879-22195e43f597');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                     text  label\n",
              "id                                                              \n",
              "265723  A group of friends began to volunteer at a hom...      0\n",
              "284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
              "207715  In 1961, Goodyear released a kit that allows P...      0\n",
              "551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
              "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
              "...                                                   ...    ...\n",
              "70046   Finish Sniper Simo H盲yh盲 during the invasion o...      0\n",
              "189377  Nigerian Prince Scam took $110K from Kansas ma...      1\n",
              "93486   Is It Safe To Smoke Marijuana During Pregnancy...      0\n",
              "140950  Julius Caesar upon realizing that everyone in ...      0\n",
              "34509   Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...      1\n",
              "\n",
              "[60000 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#display train data\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y9AwYjo_-HQ",
        "outputId": "9fca785d-48f5-436f-9328-f2582f056875"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-23c3c738-487a-4c01-962a-de8cd866c0fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>stargazer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>yeah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"Believers\" - Hezbollah 2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59146</th>\n",
              "      <td>59146</td>\n",
              "      <td>Bicycle taxi drivers of New Delhi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59147</th>\n",
              "      <td>59147</td>\n",
              "      <td>Trump blows up GOP's formula for winning House...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59148</th>\n",
              "      <td>59148</td>\n",
              "      <td>Napoleon returns from his exile on the island ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59149</th>\n",
              "      <td>59149</td>\n",
              "      <td>Deep down he always wanted to be a ballet dancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59150</th>\n",
              "      <td>59150</td>\n",
              "      <td>Toddler miraculously survives 6-story fall lan...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>59151 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23c3c738-487a-4c01-962a-de8cd866c0fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-23c3c738-487a-4c01-962a-de8cd866c0fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-23c3c738-487a-4c01-962a-de8cd866c0fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          id                                               text\n",
              "0          0                                         stargazer \n",
              "1          1                                               yeah\n",
              "2          2  PD: Phoenix car thief gets instructions from Y...\n",
              "3          3  As Trump Accuses Iran, He Has One Problem: His...\n",
              "4          4                       \"Believers\" - Hezbollah 2011\n",
              "...      ...                                                ...\n",
              "59146  59146                  Bicycle taxi drivers of New Delhi\n",
              "59147  59147  Trump blows up GOP's formula for winning House...\n",
              "59148  59148  Napoleon returns from his exile on the island ...\n",
              "59149  59149   Deep down he always wanted to be a ballet dancer\n",
              "59150  59150  Toddler miraculously survives 6-story fall lan...\n",
              "\n",
              "[59151 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#display test data\n",
        "test_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnym1-yyAFHm",
        "outputId": "ce36f9ce-5131-4aaa-af35-9d77790520d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#display the no. of nulls in each columns\n",
        "data.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqyiakyPAHnl",
        "outputId": "9d825359-d5b2-4cf7-d071-0d40d829e45e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 60000 entries, 265723 to 34509\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    60000 non-null  object\n",
            " 1   label   60000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ],
      "source": [
        "#display data information\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTNDnia4AKF2",
        "outputId": "d4ee0af0-624f-464c-c309-5ac25020a69e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "345"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check if there is duplicate in data\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7KAjKgwMH6A"
      },
      "source": [
        "##Data Cleaning and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvjasLRUAMFS"
      },
      "outputs": [],
      "source": [
        "# Drop duplicate rows\n",
        "data.drop_duplicates(subset='text', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysSwgDNlHTIO",
        "outputId": "e0f84f36-f5d2-4e19-9cda-75f64ee75879"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    31949\n",
              "1    27696\n",
              "Name: new_label, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#replace label 2 with label 1\n",
        "data[\"new_label\"] = 0\n",
        "data.loc[data[\"label\"] == 0, \"new_label\"] = 0\n",
        "data.loc[data[\"label\"] > 0, \"new_label\"] = 1\n",
        "\n",
        "#checking for the distribution of grade_bad\n",
        "data['new_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFOcMsx9AP42",
        "outputId": "42eaeb29-191d-433b-c722-0a2ec9db50bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59645, 3)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#display new data shape\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZi9P3vuAWCW",
        "outputId": "48056113-0c73-4310-e552-b8bea3b91625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from bokeh.io import output_notebook\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "import pickle\n",
        "import sklearn\n",
        "import holoviews as hv\n",
        "import nltk \n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "from pathlib import Path\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6cxIDwMAR_X",
        "outputId": "75cff31a-0580-4d8e-9ce4-77d1637d88d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#use lemmatizer to convert the word to root word\n",
        "#use stop words to remove words that not add much value to the meaning of text\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def clean_text(text, for_embedding=False):\n",
        "\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "    #use textblob to correct spelling mistakes\n",
        "    #m = TextBlob(text)\n",
        "    #text=m.correct()\n",
        "\n",
        "    #divide rows into sentences(tokens) to be easier to understod\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no lemmatizing, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL3jc0C43FqG",
        "outputId": "14098180-8df8-425f-c827-1133a2281e60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count                                                 59645\n",
              "unique                                                59645\n",
              "top       A group of friends began to volunteer at a hom...\n",
              "freq                                                      1\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.text.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9vwLFAVBr37",
        "outputId": "7f2a0c5a-f3fd-458e-d8f5-b9dd8f9ca60d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    0.535653\n",
              "1    0.464347\n",
              "Name: new_label, dtype: float64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check if train data balanced or not\n",
        "data[\"new_label\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4NvjktXCQis",
        "outputId": "5869a04a-0710-49ed-e545-e7c14a74adf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(id\n",
              " 265723    A group of friends began to volunteer at a hom...\n",
              " 284269    British Prime Minister @Theresa_May on Nerve A...\n",
              " 207715    In 1961, Goodyear released a kit that allows P...\n",
              " 551106    Happy Birthday, Bob Barker! The Price Is Right...\n",
              " 8584      Obama to Nation: 聙\"Innocent Cops and Unarmed Y...\n",
              "                                 ...                        \n",
              " 70046     Finish Sniper Simo H盲yh盲 during the invasion o...\n",
              " 189377    Nigerian Prince Scam took $110K from Kansas ma...\n",
              " 93486     Is It Safe To Smoke Marijuana During Pregnancy...\n",
              " 140950    Julius Caesar upon realizing that everyone in ...\n",
              " 34509     Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...\n",
              " Name: text, Length: 59645, dtype: object, id\n",
              " 265723    0\n",
              " 284269    0\n",
              " 207715    0\n",
              " 551106    0\n",
              " 8584      0\n",
              "          ..\n",
              " 70046     0\n",
              " 189377    1\n",
              " 93486     0\n",
              " 140950    0\n",
              " 34509     1\n",
              " Name: new_label, Length: 59645, dtype: int64, 0                                               stargazer \n",
              " 1                                                     yeah\n",
              " 2        PD: Phoenix car thief gets instructions from Y...\n",
              " 3        As Trump Accuses Iran, He Has One Problem: His...\n",
              " 4                             \"Believers\" - Hezbollah 2011\n",
              "                                ...                        \n",
              " 59146                    Bicycle taxi drivers of New Delhi\n",
              " 59147    Trump blows up GOP's formula for winning House...\n",
              " 59148    Napoleon returns from his exile on the island ...\n",
              " 59149     Deep down he always wanted to be a ballet dancer\n",
              " 59150    Toddler miraculously survives 6-story fall lan...\n",
              " Name: text, Length: 59151, dtype: object)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#split columns\n",
        "test=test_['text']\n",
        "x=data['text']\n",
        "y=data['new_label']\n",
        "x,y,test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9-N4NTGCApR"
      },
      "source": [
        "Data banaced and we don't need data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePLPCCkUMN-O"
      },
      "source": [
        "##Models Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Dlz2KECuBb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPXm1TJ6B_xC"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.model_selection import PredefinedSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tad7NpNuC2kV"
      },
      "source": [
        "###Trial 1\n",
        "Use pipeline with GridSearch, validation set and logistic regression model with word-level vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_lbfxcHCgM9",
        "outputId": "c9464b38-9663-4745-dd05-b81c35ab2f1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "9 fits failed out of a total of 18.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.82967755        nan 0.85444968        nan 0.86925323\n",
            "        nan 0.69056882        nan 0.69714448        nan 0.70115287\n",
            "        nan 0.83105665        nan 0.8567224         nan 0.87357985]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train score 0.9201564839608707\n",
            "Best params: {'cvec__ngram_range': (1, 3), 'lr__C': 1, 'lr__penalty': 'l2'}\n"
          ]
        }
      ],
      "source": [
        "# Further split the original training set to a train and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(x, y, train_size = 0.8, stratify = y, random_state = 2022)\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "split_index = [-1 if i in X_train.index else 0 for i in x.index]\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "\n",
        "#set vectorizer hyperparameters\n",
        "pipe = Pipeline([('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")),    \n",
        "                 ('lr', LogisticRegression(solver='sag'))])\n",
        "# Tune GridSearchCV\n",
        "pipe_params = {'cvec__ngram_range': [(1,1), (2,2), (1,3)],'lr__C': [0.01, 0.1,1], 'lr__penalty': ['l1', 'l2']}\n",
        "gs = GridSearchCV(pipe, param_grid=pipe_params,  scoring=\"roc_auc\", cv=pds)\n",
        "#train model with gridsearchcv\n",
        "gs.fit(x, y);\n",
        "#predict best score and params\n",
        "print(\"Train score\", gs.score(x, y))\n",
        "print(\"Best params:\", gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP5J4hW48ONe"
      },
      "source": [
        "Score is 0.83705 on kaggle\n",
        "\n",
        "I think it will stay the best score in machine learning model but in NN models the score will be better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkuVICzeDUjV"
      },
      "outputs": [],
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_['id']\n",
        "submission['label']=gs.predict_proba(test)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2V2LzCs-Pid"
      },
      "source": [
        "###Trial 2\n",
        "Use MultinominalNB with word analyzer and Gridsearch\n",
        "\n",
        "I think it will be good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il2Uw9mHTapz",
        "outputId": "47214922-408f-4b5d-f118-d491d82672cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train score 0.8961851993624514\n",
            "Best params: {'cvec__ngram_range': (1, 3), 'nb__alpha': 1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#set vectorizer hyperparameters\n",
        "pipe = Pipeline([('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")),    \n",
        "                 ('nb', MultinomialNB())])\n",
        "# Tune GridSearchCV\n",
        "pipe_params = {'cvec__ngram_range': [(1,1), (2,2), (1,3)],\n",
        "               'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "gs = GridSearchCV(estimator=pipe, param_grid=pipe_params,  scoring=\"roc_auc\", cv=pds)\n",
        "#train model with gridsearchcv\n",
        "gs.fit(x, y);\n",
        "#predict best score and params\n",
        "print(\"Train score\", gs.score(x, y))\n",
        "print(\"Best params:\", gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kztn5Wgh8aOU"
      },
      "source": [
        "Score on kaggle is: 0.82452\n",
        "\n",
        "Multinomial is good too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhVQJpR_Hnd8"
      },
      "outputs": [],
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_['id']\n",
        "submission['label']=gs.predict_proba(test)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNZrIi4fHZp_"
      },
      "source": [
        "###Trial 3\n",
        "Model with randomForest, character-level vectorizer and GridSearch with validation set\n",
        "\n",
        "I think RandomForest will overrfitt data and will not be good in test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlzzFhXOGaqa",
        "outputId": "29f1afc0-10c7-4ac5-d18d-ad84f601fd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
            "Train score 0.9797152498008315\n",
            "Best params: {'rf__max_depth': 1000, 'rf__max_leaf_nodes': None, 'rf__min_samples_split': 100, 'tvec__max_features': 2000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n"
          ]
        }
      ],
      "source": [
        "# Randomforest pipeline setup\n",
        "rf_pipe = Pipeline([('tvec',  TfidfVectorizer(preprocessor=clean_text,analyzer=\"char\", max_df=0.3, min_df=10, norm=\"l2\")),\n",
        "                    ('rf', RandomForestClassifier())])\n",
        "# Setting up randomforest params\n",
        "rf_params = {'tvec__max_features':[2000],'tvec__ngram_range': [(1, 2)],'tvec__stop_words': ['english'],\n",
        "             'rf__max_depth': [1000],'rf__min_samples_split': [100],'rf__max_leaf_nodes': [None]}\n",
        "             \n",
        "rf_pipe.fit(x, y)\n",
        "# Setting up GridSearch for TFIDFVectorizer\n",
        "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, scoring=\"roc_auc\", cv = pds, verbose = 1, n_jobs = -1)\n",
        "# Fitting Randomforest CV GS\n",
        "rf_gs.fit(x, y)\n",
        "#predict best score\n",
        "print(\"Train score\", rf_gs.score(x, y))\n",
        "print(\"Best params:\", rf_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUMynn1p8hgY"
      },
      "source": [
        "Score on kaggle is: 0.75818"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QggrjZNy-_JM"
      },
      "source": [
        "As I expected train score is 0.98 and test on kaggle is 0.72\n",
        "\n",
        "RandomForest always like to overfitt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUVOMJexGsvQ"
      },
      "outputs": [],
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_['id']\n",
        "submission['label']=rf_gs.predict_proba(test)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough3.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEnC7gcQHvzS"
      },
      "source": [
        "###Trial 4\n",
        "XGBoost Model with bayesian search and word-level vectorizer with validation set\n",
        "\n",
        "It maybe better than randomforest because overfitting problems have been solved here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OiwHcGpIvqc",
        "outputId": "9a092c7a-fa8e-4524-c405-bd7f2adfb926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n",
            "best score 0.8571246540041436\n",
            "best score {'cvec__ngram_range': (1, 3), 'my_classifier__max_depth': 50}\n",
            "CPU times: user 8min 17s, sys: 7.71 s, total: 8min 25s\n",
            "Wall time: 28min 57s\n"
          ]
        }
      ],
      "source": [
        "# combine the preprocessor with the model as a full tunable pipeline\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "full_pipline = Pipeline(steps=[('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")), \n",
        "                                 ('my_classifier', XGBClassifier(n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1))])\n",
        "full_pipline.fit(x,y)\n",
        "param_grid = {'cvec__ngram_range': [(1,1), (2,2), (1,3)],\n",
        "             'my_classifier__max_depth':[5,10, 20, 30,40,50] }\n",
        "\n",
        "grid_xg = GridSearchCV(full_pipline, param_grid=param_grid, scoring=\"roc_auc\", cv = pds, verbose = 1, n_jobs = -1)\n",
        "grid_xg.fit(x, y)\n",
        "#predict best score\n",
        "print('best score {}'.format(grid_xg.best_score_))\n",
        "print('best score {}'.format(grid_xg.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVsL9RIT8lY2"
      },
      "source": [
        "Score on kaggle is: 0.82029"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgA6k6MjJ4dK"
      },
      "outputs": [],
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_['id']\n",
        "submission['label']=grid_xg.predict_proba(test)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough4.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlyNxbOHOO3e"
      },
      "source": [
        "###Trial 5\n",
        "MLP classifier with word level vectorizer and gridsearchcv\n",
        "\n",
        "I expect in MLP the model will perform better and score will be the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJZTcc2VNvK7",
        "outputId": "c80affad-d755-41de-a78b-05841cb95afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n",
            "best score 0.8509244449869061\n",
            "best score {'cvec__ngram_range': (1, 1)}\n",
            "CPU times: user 11min 54s, sys: 9min 12s, total: 21min 7s\n",
            "Wall time: 27min 18s\n"
          ]
        }
      ],
      "source": [
        "full_pipline_5 = Pipeline(steps=[('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")), \n",
        "                                 ('my_classifier',MLPClassifier(random_state=1,solver=\"adam\",\n",
        "                                  hidden_layer_sizes=(224, 120, 12,),activation=\"relu\",n_iter_no_change=10))])\n",
        "\n",
        "param_grid_5 = {'cvec__ngram_range': [(1,1), (2,2), (1,3)]}\n",
        "grid_mlp = GridSearchCV(full_pipline_5,param_grid_5,scoring=\"roc_auc\", cv = pds, verbose = 1, n_jobs = -1)\n",
        "grid_mlp.fit(x, y)\n",
        "#predict best score\n",
        "print('best score {}'.format(grid_mlp.best_score_))\n",
        "print('best score {}'.format(grid_mlp.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMH75xmm0Yci"
      },
      "source": [
        "Score on kaggle: 0.80617"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjGlZXFS_4pq"
      },
      "source": [
        "Good score but not as much as I expect, I thought MLP will give us best score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0K44-HNBQLI-"
      },
      "outputs": [],
      "source": [
        "#predict output and save submission\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test_['id']\n",
        "submission['label']=grid_mlp.predict_proba(test)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough5.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK7jRwfC8x6v"
      },
      "source": [
        "##Problem Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSnVlHAF8375"
      },
      "source": [
        "###The Problem is:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfxVcmd58_tP"
      },
      "source": [
        "We have data contains text column and each row include title and each title will be classed as fake or not.\n",
        "the problem is that the data contains contains various forms of words so we should apply text preprocessing techniques ti clean it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJRNzRo87ru"
      },
      "source": [
        "###What is the input?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWT-e35g8_CZ"
      },
      "source": [
        "Text coulmn includes titles that will be classified as fake or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVpV02x09AkE"
      },
      "source": [
        "###What is the output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBOZ591p9Drg"
      },
      "source": [
        "Probability of how much is the title classified as fake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol2M4QCc9Du3"
      },
      "source": [
        "###What data mining function is required?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykCVgwzN9H8T"
      },
      "source": [
        "\n",
        "*   Drop duplicate\n",
        "*   Drop useless rows\n",
        "*   re.sub\n",
        "*   re.compile\n",
        "*   re.IGNORECASE\n",
        "*   word.lower()\n",
        "*   value_counts\n",
        "\n",
        "*   Steeming\n",
        "*   Stop words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM58S08o9H_1"
      },
      "source": [
        "###What could be the challenges?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzMQG6yQ9LUp"
      },
      "source": [
        "To remove operation signs from texts and stop words to make text more clear.\n",
        "To make computer understand human words, In a normal conversation between humans, things are often unsaid, whether in the form of some signal, expression, or just silence.\n",
        "Nevertheless, we, as humans, have the capacity to understand the\n",
        "underlying intent of the conversation, which a computer lacks.\n",
        "A second difficulty is owing to ambiguity in sentences. This may be at\n",
        "the word level, at the sentence level, or at the meaning level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xua63wVt9LYI"
      },
      "source": [
        "###What is the impact?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7T__PLw9Opr"
      },
      "source": [
        "The model will truly understand human language and classify if news is fake or not from it's title only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24w7kPm9OtR"
      },
      "source": [
        "###What is an ideal solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-2XMXCH9Rh5"
      },
      "source": [
        "Logistic regression model with word-level victorizer and GridSearchCV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvp0Eu7j9YFs"
      },
      "source": [
        "###What is the experimental protocol used and how was it carried out?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWx3uv0d9Y9K"
      },
      "source": [
        "tf-idf Char level vectorizer and tf-idf Word level vectorizer.\n",
        "Both are good and each one of them is good with sort of data.\n",
        "We cannot decide which is better than other, It depends only on trained data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnhifUxY9ZGN"
      },
      "source": [
        "###What preprocessing steps are used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0q3aai9b6l"
      },
      "source": [
        "*   We dropped duplicate rows and clear data that includes false label.\n",
        "*   We cleared stop words.\n",
        "*   We cleared operations signs to make classifying easier.\n",
        "*   Convert upper cases to lower.\n",
        "*   Use tf-idf with word vectorizer and char vectorizer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6r0fsvU8qjW"
      },
      "source": [
        "##Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9It5EafC8bpo"
      },
      "source": [
        "\n",
        "###🌈 What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENz4-GSF8tYB"
      },
      "source": [
        "Character n-gram compute how much charachter repeated depends on the selected number, but word n-gram compute how much word repeated depends on selected number.\n",
        "\n",
        "Character Tokenizers handles OOV words coherently by preserving the information of the word. It breaks down the OOV word into characters and represents the word in terms of these characters. It also limits the size of the vocabulary. since the 26 vocabulary contains a unique set of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XtrLSm18dAb"
      },
      "source": [
        "###🌈 What is the difference between stop word removal and stemming? Are these techniques language-dependent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYSrr0DJ8uDi"
      },
      "source": [
        "Stop word removes words that not add much value to the meaning of the text.\n",
        "\n",
        "like (“the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc.).\n",
        "\n",
        "Steeming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word.\n",
        "\n",
        "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
        "\n",
        "It is highly dependent on the task we are performing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4uoeLOV8gI3"
      },
      "source": [
        "###🌈 Is tokenization techniques language dependent? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVgaX2KF8um6"
      },
      "source": [
        "Yes, Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTm3tgrx8jEh"
      },
      "source": [
        "###🌈 What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3nto8u8vQf"
      },
      "source": [
        "Count Vectorizer is a way to convert a given set of strings into a frequency representation.\n",
        "\n",
        "TF-IDF stands for Term Frequency — Inverse Document Frequency and is a statistic that aims to better define how important a word is for a document, while also taking into account the relation to other documents from the same corpus.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzaCxmpQy-fX"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odZzsWGQy6OP"
      },
      "source": [
        "https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv30gQeYxIq2"
      },
      "source": [
        "https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9AcZaPsxBB3"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QwTx3-GjL6ZF",
        "vK7jRwfC8x6v",
        "F6r0fsvU8qjW"
      ],
      "name": "Reddit_Fake_Post_Assign3dm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}